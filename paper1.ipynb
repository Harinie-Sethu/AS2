{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pickle dataset files\n",
    "train = pickle.load(open('wiki_qa_train.pkl', 'rb'))\n",
    "test = pickle.load(open('wiki_qa_test.pkl', 'rb'))\n",
    "val = pickle.load(open('wiki_qa_validation.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data is a function defined in embeds_and_cosinet.ipynb\n",
    "wiki_qa_train = tokenize_data(train)\n",
    "wiki_qa_test = tokenize_data(test)\n",
    "wiki_qa_validation = tokenize_data(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_embeddings = torch.load('wiki_qa_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in wiki_qa_train is of the form \n",
    "# (question, answer, label) where for question and answer we have the w2i representation\n",
    "\n",
    "def get_embedded_forms(data):\n",
    "    for i, datum in enumerate(data):\n",
    "        data[i][\"question\"] = [wiki_qa_embeddings[index] for index in datum[\"question\"]]\n",
    "        data[i][\"answer\"] = [[wiki_qa_embeddings[index] for index in datum[\"answer\"]] for answer in datum[\"answer\"]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_embedded = get_embedded_forms(wiki_qa_train)\n",
    "test_embedded = get_embedded_forms(wiki_qa_test)\n",
    "validation_embedded = get_embedded_forms(wiki_qa_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_num_of_pairs(data):\n",
    "    max_pairs = 0\n",
    "    max_length = 0\n",
    "    for datum in data:\n",
    "        if len(datum[\"answer\"]) > max_pairs:\n",
    "            max_pairs = len(datum[\"answer\"])\n",
    "    return max_pairs\n",
    "\n",
    "max_pairs = max(max_num_of_pairs(train_embedded), max_num_of_pairs(test_embedded), max_num_of_pairs(validation_embedded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sent_length(data):\n",
    "    max_length = 0\n",
    "    for datum in data:\n",
    "        for answer in datum[\"answer\"]:\n",
    "            if len(answer) > max_length:\n",
    "                max_length = len(answer)\n",
    "    return max_length\n",
    "\n",
    "max_length = max(max_sent_length(train_embedded), max_sent_length(test_embedded), max_sent_length(validation_embedded)) + 1 # +1 for the cosine similarity that'll be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_cosine(question, answer):\n",
    "    question_r = []\n",
    "    answer_r = []\n",
    "\n",
    "    for word in question:\n",
    "        max_similarity = 0\n",
    "        for word2 in answer:\n",
    "            similarity = torch.cosine_similarity(word, word2)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        question_r.append(max_similarity)\n",
    "    \n",
    "    for word in answer:\n",
    "        max_similarity = 0\n",
    "        for word2 in question:\n",
    "            similarity = torch.cosine_similarity(word, word2)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        answer_r.append(max_similarity)\n",
    "    \n",
    "    return torch.tensor(question_r), torch.tensor(answer_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_embeddings(question, answer):\n",
    "    question_r, answer_r = get_pair_cosine(question, answer)\n",
    "    for i in range(len(question)):\n",
    "        question[i] = torch.cat((question[i], question_r[i].unsqueeze(0)), 0)\n",
    "    for i in range(len(answer)):\n",
    "        answer[i] = torch.cat((answer[i], answer_r[i].unsqueeze(0)), 0)\n",
    "    \n",
    "    if len(question) < max_length:\n",
    "        for i in range(max_length - len(question)):\n",
    "            question.append(torch.zeros(301))\n",
    "    \n",
    "    if len(answer) < max_length:\n",
    "        for i in range(max_length - len(answer)):\n",
    "            answer.append(torch.zeros(301))\n",
    "            \n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might want to find better ways to implement padding for sentence as well as for number of sentence pairs\n",
    "filler = [torch.zeros(301) for i in range(max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answer_pairs(data):\n",
    "    pairs = []\n",
    "    question_pairs = []\n",
    "    for datum in data:\n",
    "        question = datum[\"question\"]\n",
    "        for i,answer in enumerate(datum[\"answer\"]):\n",
    "            question, answer = extended_embeddings(question, answer)\n",
    "            question_pairs.append((question, answer))\n",
    "        pairs.append(question_pairs)\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating modified embeddings along with cosine similarity, this is being generated here because we want to keep them static as per the paper\n",
    "train_pairs = get_question_answer_pairs(train_embedded)\n",
    "test_pairs = get_question_answer_pairs(test_embedded)\n",
    "validation_pairs = get_question_answer_pairs(validation_embedded)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
