{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kapil\\anaconda3\\envs\\anlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokenize_sentence(sentence):\n",
    "    if sentence == \"\":\n",
    "        return sentence\n",
    "    sentence = re.sub(r'\\n', \" \", sentence)\n",
    "    sentence = re.sub(r'(((http|https):\\/\\/)|www\\.)([a-zA-Z0-9]+\\.){0,2}[a-zA-Z0-9]+([a-zA-Z0-9\\/#%&=\\?_\\.\\-\\+]+)', \"\", sentence)\n",
    "    sentence = re.sub(r'(@[a-zA-Z0-9_]+)', \"\", sentence)\n",
    "    sentence = re.sub(r'(#[a-zA-Z0-9_]+\\b)', \"\", sentence)\n",
    "    sentence = re.sub(r'\\d+', \"\", sentence)\n",
    "    sentence = re.sub(r'--', \" \", sentence)\n",
    "    sentence = re.sub(r'[\\_\\$\\*\\^\\(\\)\\[\\]\\{\\}\\=\\+\\<\\>\",\\&\\%\\-\\—\\”\\“\\–\\\\\\.\\?\\!;]', \" \", sentence)\n",
    "    sentence = re.sub(r'&lt;[^&]*&gt;', '', sentence)\n",
    "    sentence = re.sub(r'——', \"...\", sentence)\n",
    "    sentence = re.sub(r'—-?', r\", \", sentence)\n",
    "    sentence = re.sub(r'_', \"\", sentence)\n",
    "    sentence = re.sub(r'[“”]', \"\\\"\", sentence)\n",
    "    sentence = re.sub(r'[‘’]', \"\\'\", sentence)\n",
    "    sentence = re.sub(r'[.,\\'\"!?()\\[\\]{}\\-;:]+', \"\", sentence)\n",
    "    sentence = re.sub(r'/', \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "    pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "    sentence = [word for word in sentence if pattern.match(word)]\n",
    "\n",
    "    # pattern2 = r\"/\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki_QA\n",
    "\n",
    "#### Link to tokenized dataset (pkl) \n",
    "- Train: https://drive.google.com/file/d/1S5HtkMnENY7KnxHxZG5cn0-tfaCqMfvt/view?usp=sharing\n",
    "- Validation: https://drive.google.com/file/d/1sEVb9BmZKjG3M8-xcI65WLvtWkshJRU8/view?usp=sharing\n",
    "- Test: https://drive.google.com/file/d/1-CjZ-j7JMShlXj06gZoySQE9RD_umE79/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE TO GENERATE TOKENIZED DATASETS IS PROVIDED BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_dataset = load_dataset(\"wiki_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 'Q2',\n",
       " 'question': 'How are the directions of the velocity and force vectors related in a circular motion',\n",
       " 'document_title': 'Circular motion',\n",
       " 'answer': \"Without this acceleration, the object would move in a straight line, according to Newton's laws of motion .\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_qa_dataset[\"train\"][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_set = {\n",
    "    \"train\" : {},\n",
    "    \"validation\" : {},\n",
    "    \"test\" : {}\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for example in wiki_qa_dataset[split]:\n",
    "        if example[\"question_id\"] not in wiki_qa_set[split]:\n",
    "            wiki_qa_set[split][example[\"question_id\"]] = {\n",
    "                \"question\" : preprocess_tokenize_sentence(example[\"question\"]),\n",
    "                \"answers\" : [],\n",
    "                \"labels\" : [],\n",
    "                \"sum_labels\" : 0\n",
    "            }\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"answers\"].append(preprocess_tokenize_sentence(example[\"answer\"]))\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"labels\"].append(example[\"label\"])\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"sum_labels\"] += example[\"label\"]\n",
    "\n",
    "wiki_qa_train = [{\"question\" : wiki_qa_set[\"train\"][qn][\"question\"], \n",
    "                  \"answers\" : wiki_qa_set[\"train\"][qn][\"answers\"], \n",
    "                  \"labels\" : wiki_qa_set[\"train\"][qn][\"labels\"]} for qn in wiki_qa_set[\"train\"] if wiki_qa_set[\"train\"][qn][\"sum_labels\"] > 0]\n",
    "\n",
    "wiki_qa_validation = [{\"question\" : wiki_qa_set[\"validation\"][qn][\"question\"],\n",
    "                    \"answers\" : wiki_qa_set[\"validation\"][qn][\"answers\"],\n",
    "                    \"labels\" : wiki_qa_set[\"validation\"][qn][\"labels\"]} for qn in wiki_qa_set[\"validation\"] if wiki_qa_set[\"validation\"][qn][\"sum_labels\"] > 0]\n",
    "\n",
    "wiki_qa_test = [{\"question\" : wiki_qa_set[\"test\"][qn][\"question\"],\n",
    "                \"answers\" : wiki_qa_set[\"test\"][qn][\"answers\"],\n",
    "                \"labels\" : wiki_qa_set[\"test\"][qn][\"labels\"]} for qn in wiki_qa_set[\"test\"] if wiki_qa_set[\"test\"][qn][\"sum_labels\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['how', 'are', 'glacier', 'caves', 'formed'], 'answers': [['a', 'partly', 'submerged', 'glacier', 'cave', 'on', 'perito', 'moreno', 'glacier'], ['the', 'ice', 'facade', 'is', 'approximately', 'm', 'high'], ['ice', 'formations', 'in', 'the', 'titlis', 'glacier', 'cave'], ['a', 'glacier', 'cave', 'is', 'a', 'cave', 'formed', 'within', 'the', 'ice', 'of', 'a', 'glacier'], ['glacier', 'caves', 'are', 'often', 'called', 'ice', 'caves', 'but', 'this', 'term', 'is', 'properly', 'used', 'to', 'describe', 'bedrock', 'caves', 'that', 'contain', 'year', 'round', 'ice']], 'labels': [0, 0, 0, 1, 0]}\n",
      "{'question': ['how', 'much', 'is', 'tablespoon', 'of', 'water'], 'answers': [['this', 'tablespoon', 'has', 'a', 'capacity', 'of', 'about', 'ml'], ['measuring', 'spoons'], ['in', 'the', 'us', 'and', 'parts', 'of', 'canada', 'a', 'tablespoon', 'is', 'the', 'largest', 'type', 'of', 'spoon', 'used', 'for', 'eating', 'from', 'a', 'bowl'], ['in', 'the', 'uk', 'europe', 'and', 'most', 'commonwealth', 'countries', 'a', 'tablespoon', 'is', 'a', 'type', 'of', 'large', 'spoon', 'usually', 'used', 'for', 'serving'], ['in', 'countries', 'where', 'a', 'tablespoon', 'is', 'a', 'serving', 'spoon', 'the', 'nearest', 'equivalent', 'to', 'the', 'us', 'tablespoon', 'is', 'either', 'the', 'dessert', 'spoon', 'or', 'the', 'soup', 'spoon'], ['a', 'tablespoonful', 'nominally', 'the', 'capacity', 'of', 'one', 'tablespoon', 'is', 'commonly', 'used', 'as', 'a', 'measure', 'of', 'volume', 'in', 'cooking'], ['it', 'is', 'abbreviated', 'as', 't', 'tb', 'tbs', 'tbsp', 'tblsp', 'or', 'tblspn'], ['the', 'capacity', 'of', 'ordinary', 'tablespoons', 'is', 'not', 'regulated', 'by', 'law', 'and', 'is', 'subject', 'to', 'considerable', 'variation'], ['in', 'the', 'usa', 'one', 'tablespoon', 'measurement', 'unit', 'is', 'approximately', 'ml', 'the', 'capacity', 'of', 'an', 'actual', 'tablespoon', 'dining', 'utensil', 'ranges', 'from', 'ml', 'to', 'ml'], ['in', 'australia', 'one', 'tablespoon', 'measurement', 'unit', 'is', 'ml']], 'labels': [1, 0, 0, 0, 0, 0, 0, 0, 1, 1]}\n",
      "{'question': ['how', 'much', 'are', 'the', 'harry', 'potter', 'movies', 'worth'], 'answers': [['harry', 'potter', 'is', 'a', 'series', 'of', 'seven', 'fantasy', 'novels', 'written', 'by', 'the', 'british', 'author', 'j', 'k', 'rowling'], ['the', 'books', 'chronicle', 'the', 'adventures', 'of', 'a', 'wizard', 'harry', 'potter', 'and', 'his', 'friends', 'ronald', 'weasley', 'and', 'hermione', 'granger', 'all', 'of', 'whom', 'are', 'students', 'at', 'hogwarts', 'school', 'of', 'witchcraft', 'and', 'wizardry'], ['the', 'main', 'story', 'arc', 'concerns', 'harrys', 'quest', 'to', 'overcome', 'the', 'dark', 'wizard', 'lord', 'voldemort', 'whose', 'aims', 'are', 'to', 'become', 'immortal', 'conquer', 'the', 'wizarding', 'world', 'subjugate', 'non', 'magical', 'people', 'and', 'destroy', 'all', 'those', 'who', 'stand', 'in', 'his', 'way', 'especially', 'harry', 'potter'], ['since', 'the', 'release', 'of', 'the', 'first', 'novel', 'harry', 'potter', 'and', 'the', 'philosophers', 'stone', 'on', 'june', 'the', 'books', 'have', 'gained', 'immense', 'popularity', 'critical', 'acclaim', 'and', 'commercial', 'success', 'worldwide'], ['the', 'series', 'has', 'also', 'had', 'some', 'share', 'of', 'criticism', 'including', 'concern', 'for', 'the', 'increasingly', 'dark', 'tone'], ['the', 'book', 'series', 'has', 'sold', 'about', 'million', 'copies', 'making', 'it', 'the', 'best', 'selling', 'book', 'series', 'in', 'history', 'and', 'has', 'been', 'translated', 'into', 'languages'], ['the', 'last', 'four', 'books', 'consecutively', 'set', 'records', 'as', 'the', 'fastest', 'selling', 'books', 'in', 'history'], ['a', 'series', 'of', 'many', 'genres', 'including', 'fantasy', 'and', 'coming', 'of', 'age', 'with', 'elements', 'of', 'mystery', 'thriller', 'adventure', 'and', 'romance', 'it', 'has', 'many', 'cultural', 'meanings', 'and', 'references'], ['according', 'to', 'rowling', 'the', 'main', 'theme', 'is', 'death'], ['there', 'are', 'also', 'many', 'other', 'themes', 'in', 'the', 'series', 'such', 'as', 'prejudice', 'and', 'corruption'], ['the', 'initial', 'major', 'publishers', 'of', 'the', 'books', 'were', 'bloomsbury', 'in', 'the', 'united', 'kingdom', 'and', 'scholastic', 'press', 'in', 'the', 'united', 'states'], ['the', 'books', 'have', 'since', 'been', 'published', 'by', 'many', 'publishers', 'worldwide'], ['the', 'books', 'with', 'the', 'seventh', 'book', 'split', 'into', 'two', 'parts', 'have', 'been', 'made', 'into', 'an', 'eight', 'part', 'film', 'series', 'by', 'warner', 'bros', 'pictures', 'the', 'highest', 'grossing', 'film', 'series', 'of', 'all', 'time'], ['the', 'series', 'also', 'originated', 'much', 'tie', 'in', 'merchandise', 'making', 'the', 'harry', 'potter', 'brand', 'worth', 'in', 'excess', 'of', 'billion'], ['also', 'due', 'to', 'the', 'success', 'of', 'the', 'books', 'and', 'films', 'harry', 'potter', 'has', 'been', 'used', 'for', 'a', 'theme', 'park', 'the', 'wizarding', 'world', 'of', 'harry', 'potter', 'in', 'universal', 'parks', 'resorts', 's', 'islands', 'of', 'adventure']], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}\n",
      "{'question': ['how', 'a', 'rocket', 'engine', 'works'], 'answers': [['rs', 'being', 'tested', 'at', 'nasas', 'stennis', 'space', 'center'], ['the', 'nearly', 'transparent', 'exhaust', 'is', 'due', 'to', 'this', 'engines', 'exhaust', 'being', 'mostly', 'superheated', 'steam', 'water', 'vapor', 'from', 'its', 'propellants', 'hydrogen', 'and', 'oxygen'], ['viking', 'c', 'rocket', 'engine'], ['a', 'rocket', 'engine', 'or', 'simply', 'rocket', 'is', 'a', 'jet', 'engine', 'that', 'uses', 'only', 'stored', 'propellant', 'mass', 'for', 'forming', 'its', 'high', 'speed', 'propulsive', 'jet'], ['rocket', 'engines', 'are', 'reaction', 'engines', 'and', 'obtain', 'thrust', 'in', 'accordance', 'with', 'newtons', 'third', 'law'], ['since', 'they', 'need', 'no', 'external', 'material', 'to', 'form', 'their', 'jet', 'rocket', 'engines', 'can', 'be', 'used', 'for', 'spacecraft', 'propulsion', 'as', 'well', 'as', 'terrestrial', 'uses', 'such', 'as', 'missiles'], ['most', 'rocket', 'engines', 'are', 'internal', 'combustion', 'engines', 'although', 'non', 'combusting', 'forms', 'also', 'exist'], ['rocket', 'engines', 'as', 'a', 'group', 'have', 'the', 'highest', 'exhaust', 'velocities', 'are', 'by', 'far', 'the', 'lightest', 'but', 'are', 'the', 'least', 'propellant', 'efficient', 'of', 'all', 'types', 'of', 'jet', 'engines']], 'labels': [0, 0, 0, 1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(wiki_qa_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki_qa_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wiki_qa_train, f)\n",
    "\n",
    "with open(\"wiki_qa_validation.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wiki_qa_validation, f)\n",
    "\n",
    "with open(\"wiki_qa_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(wiki_qa_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is tokenized before storing in pickle form, hence no tokenization is required when you load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD\n",
    "\n",
    "#### Link to tokenized dataset (pkl)\n",
    "- Train: https://drive.google.com/file/d/1_OSiMwCTvaeoziuK2Ew0U4dHeUfVIK6f/view?usp=sharing\n",
    "- Validation: https://drive.google.com/file/d/15lPDZODuySawp9HLpRuVssaJ6m6XTPvA/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DON'T TOUCH THIS SECTION UNLESS YOU WANT TO REMAKE THE SQUAD TRAIN DATASET FILE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [26:46<00:00, 54.53it/s] \n"
     ]
    }
   ],
   "source": [
    "squad_train_set = []\n",
    "for example in tqdm(squad_dataset[\"train\"]):\n",
    "    tmp = {}\n",
    "    tmp[\"question\"] = example[\"question\"]\n",
    "    con_text = nlp(example[\"context\"])\n",
    "    tmp[\"context\"] = [token.text for token in con_text.sents]\n",
    "    tmp[\"labels\"] = [0 for _ in range(len(tmp[\"context\"]))]\n",
    "    for sent in tmp[\"context\"]:\n",
    "        if example[\"answers\"][\"text\"][0] in sent:\n",
    "            tmp[\"labels\"][tmp[\"context\"].index(sent)] = 1\n",
    "    squad_train_set.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [07:59<00:00, 22.06it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_valid_set = []\n",
    "for example in tqdm(squad_dataset[\"validation\"]):\n",
    "    tmp = {}\n",
    "    tmp[\"question\"] = example[\"question\"]\n",
    "    con_text = nlp(example[\"context\"])\n",
    "    tmp[\"context\"] = [token.text for token in con_text.sents]\n",
    "    tmp[\"labels\"] = [0 for _ in range(len(tmp[\"context\"]))]\n",
    "    for sent in tmp[\"context\"]:\n",
    "        if example[\"answers\"][\"text\"][0] in sent:\n",
    "            tmp[\"labels\"][tmp[\"context\"].index(sent)] = 1\n",
    "    squad_valid_set.append(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [02:46, 526.65it/s]\n",
      "10570it [00:19, 529.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, elem in tqdm(enumerate(squad_train_set)):\n",
    "    elem[\"question\"] = preprocess_tokenize_sentence(elem[\"question\"])\n",
    "    elem[\"context\"] = [preprocess_tokenize_sentence(sent) for sent in elem[\"context\"]]\n",
    "\n",
    "for i, elem in tqdm(enumerate(squad_valid_set)):\n",
    "    elem[\"question\"] = preprocess_tokenize_sentence(elem[\"question\"])\n",
    "    elem[\"context\"] = [preprocess_tokenize_sentence(sent) for sent in elem[\"context\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing in pickle form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"squad_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(squad_train_set, f)\n",
    "\n",
    "with open(\"squad_valid.pkl\", \"wb\") as f:\n",
    "    pickle.dump(squad_valid_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If dataset pkl file exists then continue from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"squad_train.pkl\", \"rb\") as f:\n",
    "    squad_train_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_valid_set = pickle.load(open(\"squad_valid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', 'in', 'lourdes', 'france'], 'context': [['architecturally', 'the', 'school', 'has', 'a', 'catholic', 'character'], ['atop', 'the', 'main', 'buildings', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary'], ['immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', 'venite', 'ad', 'me', 'omnes'], ['next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart'], ['immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection'], ['it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in'], ['at', 'the', 'end', 'of', 'the', 'main', 'drive', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', 'statues', 'and', 'the', 'gold', 'dome', 'is', 'a', 'simple', 'modern', 'stone', 'statue', 'of', 'mary']], 'labels': [0, 0, 0, 0, 0, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(squad_train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['which', 'nfl', 'team', 'represented', 'the', 'afc', 'at', 'super', 'bowl'], 'context': [['super', 'bowl', 'was', 'an', 'american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', 'nfl', 'for', 'the', 'season'], ['the', 'american', 'football', 'conference', 'afc', 'champion', 'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', 'nfc', 'champion', 'carolina', 'panthers', 'to', 'earn', 'their', 'third', 'super', 'bowl', 'title'], ['the', 'game', 'was', 'played', 'on', 'february', 'at', 'levis', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', 'california'], ['as', 'this', 'was', 'the', 'th', 'super', 'bowl', 'the', 'league', 'emphasized', 'the', 'golden', 'anniversary', 'with', 'various', 'gold', 'themed', 'initiatives', 'as', 'well', 'as', 'temporarily', 'suspending', 'the', 'tradition', 'of', 'naming', 'each', 'super', 'bowl', 'game', 'with', 'roman', 'numerals', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', 'as', 'super', 'bowl', 'l', 'so', 'that', 'the', 'logo', 'could', 'prominently', 'feature', 'the', 'arabic', 'numerals']], 'labels': [0, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(squad_valid_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qnli (to be used only for TANDA - has not been recommended in the Cosinet paper)\n",
    "\n",
    "Further preprocessing of QNLI to be done later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_dataset = load_dataset(\"glue\", \"qnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who replaced Stratford Canning after he first resigned as British ambassador to the Ottoman Empire ?',\n",
       " 'sentence': \"In February 1853, the British government of Lord Aberdeen, the prime minister, re-appointed Stratford Canning as British ambassador to the Ottoman Empire.:110 Having resigned the ambassadorship in January, he had been replaced by Colonel Rose as chargé d'affaires.\",\n",
       " 'label': 0,\n",
       " 'idx': 2500}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnli_dataset[\"train\"][2500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept-Net Numberbatch embeddings : obtained using `numberbatch-en.txt` from https://github.com/commonsense/conceptnet-numberbatch#downloads\n",
    "\n",
    "#### Link to embeddings (pkl)\n",
    "https://drive.google.com/file/d/1f2urijZCLm0KYHgnF9bLsR8ABfPmR6Nj/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_embeds = pd.read_csv(\"numberbatch-en.txt\", sep=\" \", skiprows=1, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>##</th>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.0405</td>\n",
       "      <td>-0.0341</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>-0.0575</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>-0.0145</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0443</td>\n",
       "      <td>-0.0217</td>\n",
       "      <td>-0.0239</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>###</th>\n",
       "      <td>0.0202</td>\n",
       "      <td>-0.0249</td>\n",
       "      <td>-0.0653</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>-0.0923</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>-0.0334</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>-0.0285</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0418</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>-0.0575</td>\n",
       "      <td>-0.0580</td>\n",
       "      <td>0.0651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>####</th>\n",
       "      <td>0.0521</td>\n",
       "      <td>-0.0262</td>\n",
       "      <td>-0.0881</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>-0.1168</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>-0.0287</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>-0.0342</td>\n",
       "      <td>0.0570</td>\n",
       "      <td>-0.0424</td>\n",
       "      <td>-0.0192</td>\n",
       "      <td>0.0599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#####</th>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>-0.0388</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>-0.0617</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>-0.0287</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0399</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>-0.0622</td>\n",
       "      <td>-0.0117</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>-0.0165</td>\n",
       "      <td>0.0645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#####_metres</th>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-0.0410</td>\n",
       "      <td>-0.0531</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>-0.1031</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>-0.0391</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0401</td>\n",
       "      <td>-0.0299</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.0354</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>-0.0084</td>\n",
       "      <td>-0.0431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>⠙_⠉_⠇</th>\n",
       "      <td>-0.0873</td>\n",
       "      <td>-0.1373</td>\n",
       "      <td>-0.0600</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>-0.0224</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>-0.1467</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>-0.1169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0158</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>-0.0386</td>\n",
       "      <td>0.0933</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>-0.0170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>⠨_⠴</th>\n",
       "      <td>-0.0140</td>\n",
       "      <td>-0.0175</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>-0.0768</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>-0.1958</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0409</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>-0.0302</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>-0.0403</td>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>-0.0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>形容詞</th>\n",
       "      <td>-0.0356</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>-0.1663</td>\n",
       "      <td>-0.1416</td>\n",
       "      <td>-0.0936</td>\n",
       "      <td>-0.1030</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>-0.0403</td>\n",
       "      <td>-0.0883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0665</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>-0.0254</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0191</td>\n",
       "      <td>-0.0744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>江</th>\n",
       "      <td>0.0455</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>-0.0919</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>-0.1176</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>-0.0135</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>을</th>\n",
       "      <td>-0.1069</td>\n",
       "      <td>-0.0508</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0603</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>-0.0169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0079</td>\n",
       "      <td>-0.0228</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>-0.0367</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>-0.0186</td>\n",
       "      <td>-0.0050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>516782 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1       2       3       4       5       6       7       8    \\\n",
       "0                                                                              \n",
       "##            0.0295 -0.0405 -0.0341  0.0837 -0.0575  0.0482 -0.0145  0.0019   \n",
       "###           0.0202 -0.0249 -0.0653  0.0930 -0.0923  0.0306 -0.0093  0.0224   \n",
       "####          0.0521 -0.0262 -0.0881  0.1085 -0.1168  0.0324  0.0084  0.0382   \n",
       "#####         0.0416  0.0061 -0.0388  0.0175 -0.0617 -0.0043  0.0140  0.0725   \n",
       "#####_metres -0.0018 -0.0410 -0.0531  0.1115 -0.1031 -0.0065 -0.0391  0.0145   \n",
       "...              ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "⠙_⠉_⠇        -0.0873 -0.1373 -0.0600  0.0142 -0.0113 -0.0224  0.0531 -0.1467   \n",
       "⠨_⠴          -0.0140 -0.0175  0.0094  0.0208 -0.0768 -0.0918  0.0447 -0.1958   \n",
       "形容詞          -0.0356  0.0516 -0.1663 -0.1416 -0.0936 -0.1030  0.1375 -0.1280   \n",
       "江             0.0455 -0.0179 -0.0083 -0.0100 -0.0919  0.0961  0.0035 -0.0000   \n",
       "을            -0.1069 -0.0508  0.0503  0.1519  0.1394 -0.0603  0.0633 -0.0902   \n",
       "\n",
       "                 9       10   ...     291     292     293     294     295  \\\n",
       "0                             ...                                           \n",
       "##            0.0347  0.0825  ...  0.0095  0.0908  0.0549  0.0121  0.0056   \n",
       "###          -0.0334  0.0750  ... -0.0078  0.0819  0.0082 -0.0285  0.0007   \n",
       "####         -0.0287  0.1098  ...  0.0540  0.0769  0.0234 -0.0080  0.1110   \n",
       "#####        -0.0287  0.0469  ...  0.0255  0.0511 -0.0039 -0.0399  0.0665   \n",
       "#####_metres  0.0391  0.1635  ... -0.0401 -0.0299 -0.0179 -0.0354  0.0237   \n",
       "...              ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "⠙_⠉_⠇         0.0259 -0.1169  ... -0.0158  0.0006  0.0229  0.0023 -0.0089   \n",
       "⠨_⠴           0.0033  0.0013  ... -0.0409  0.0254  0.0609  0.0043 -0.0302   \n",
       "形容詞          -0.0403 -0.0883  ... -0.0665  0.0036  0.0352  0.0578  0.0834   \n",
       "江             0.0138  0.0699  ...  0.0083  0.0978 -0.1176  0.0280 -0.0135   \n",
       "을             0.0182 -0.0169  ... -0.0079 -0.0228 -0.0012 -0.0020  0.1053   \n",
       "\n",
       "                 296     297     298     299     300  \n",
       "0                                                     \n",
       "##            0.0443 -0.0217 -0.0239  0.0403 -0.0005  \n",
       "###          -0.0418  0.0893 -0.0575 -0.0580  0.0651  \n",
       "####         -0.0342  0.0570 -0.0424 -0.0192  0.0599  \n",
       "#####        -0.0622 -0.0117  0.0066 -0.0165  0.0645  \n",
       "#####_metres  0.0357  0.0335  0.0124 -0.0084 -0.0431  \n",
       "...              ...     ...     ...     ...     ...  \n",
       "⠙_⠉_⠇        -0.0462 -0.0386  0.0933  0.0538 -0.0170  \n",
       "⠨_⠴          -0.0257 -0.0403  0.0951  0.0355 -0.0181  \n",
       "形容詞          -0.0254  0.0163 -0.0009 -0.0191 -0.0744  \n",
       "江             0.0138  0.0387  0.0450  0.0528  0.0087  \n",
       "을             0.0185 -0.0367 -0.0099 -0.0186 -0.0050  \n",
       "\n",
       "[516782 rows x 300 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeds.loc[\"the\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THIS SECTION TO STORE THE EMBEDDINGS IN A PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"numberbatch-en.txt\", \"r\")\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "for line in f:\n",
    "    line = line.split(\" \")\n",
    "    word = line[0]\n",
    "    embed = np.asarray(line[1:], dtype=\"float32\")\n",
    "    embeddings[word] = embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embeddings from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"numberbatch_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1265, -0.0897, -0.1441, -0.0691,  0.0024, -0.0428,  0.1635,\n",
       "        0.1092,  0.0225, -0.0023, -0.1066,  0.0672,  0.0165, -0.0922,\n",
       "        0.0828, -0.0673,  0.0214,  0.1012, -0.1139, -0.0741,  0.0493,\n",
       "        0.0114, -0.0317,  0.1188, -0.0103, -0.052 , -0.0403, -0.0175,\n",
       "        0.0645,  0.012 , -0.0525, -0.0848,  0.0659, -0.0299,  0.0017,\n",
       "        0.0488,  0.0374, -0.1365,  0.0575,  0.1142,  0.0156, -0.0408,\n",
       "        0.0027,  0.0862, -0.0022,  0.0215,  0.0024, -0.0432, -0.057 ,\n",
       "        0.0345,  0.0279, -0.0385,  0.0017,  0.1389, -0.0318, -0.0322,\n",
       "       -0.0802, -0.0694, -0.0643,  0.0778,  0.0747, -0.0437, -0.0723,\n",
       "        0.0352,  0.0326,  0.0429, -0.0019,  0.1215, -0.0162,  0.0679,\n",
       "        0.0049,  0.115 , -0.1836,  0.1341,  0.0997, -0.0246, -0.0155,\n",
       "       -0.025 ,  0.0566, -0.0245,  0.0154,  0.0234,  0.0168,  0.021 ,\n",
       "        0.1137,  0.0786, -0.1198,  0.0025,  0.0605,  0.1162, -0.0091,\n",
       "       -0.0163, -0.046 , -0.0303, -0.0147, -0.0548,  0.0414, -0.1429,\n",
       "       -0.0026, -0.0812,  0.0457,  0.0024,  0.0144, -0.1039,  0.1198,\n",
       "       -0.0267, -0.0724,  0.0555,  0.0088,  0.0695, -0.0946, -0.0778,\n",
       "        0.0511, -0.089 ,  0.1055,  0.0144,  0.0308,  0.0942,  0.0661,\n",
       "        0.0587, -0.0082, -0.0241,  0.0491, -0.0459, -0.0372, -0.0499,\n",
       "       -0.0517, -0.0488,  0.0613,  0.0099,  0.0119, -0.0063, -0.0353,\n",
       "        0.0277, -0.0209, -0.0036,  0.0237,  0.0441,  0.0274, -0.0441,\n",
       "       -0.1218, -0.0151,  0.1097, -0.1123,  0.0205, -0.0108, -0.0398,\n",
       "        0.0046, -0.0072,  0.0908, -0.0263, -0.0468, -0.0244, -0.0175,\n",
       "        0.0196, -0.0035,  0.0325, -0.0314, -0.0474, -0.0045, -0.0229,\n",
       "        0.0304,  0.0923,  0.0336, -0.0012,  0.0255, -0.0277,  0.0118,\n",
       "        0.0229, -0.0601,  0.0408,  0.0081, -0.1122,  0.0002,  0.0383,\n",
       "       -0.0147, -0.0149, -0.0548,  0.0069,  0.0144, -0.023 , -0.0127,\n",
       "       -0.061 , -0.0101, -0.0468,  0.0543,  0.0483, -0.0395,  0.0244,\n",
       "        0.0252, -0.0086,  0.0329,  0.0022, -0.0039,  0.0961,  0.0278,\n",
       "        0.0399,  0.0967,  0.103 ,  0.0522, -0.0043,  0.0622, -0.0883,\n",
       "       -0.0175,  0.0006, -0.0498, -0.0417,  0.063 , -0.0175,  0.0758,\n",
       "       -0.0661, -0.0648,  0.0182,  0.0563,  0.0761, -0.0592, -0.0229,\n",
       "       -0.0142,  0.102 , -0.0024, -0.0085,  0.0324, -0.0056, -0.0485,\n",
       "        0.0838, -0.044 ,  0.0098, -0.0431, -0.0417,  0.0911, -0.0106,\n",
       "       -0.1205, -0.0689,  0.0122, -0.004 , -0.0179,  0.058 ,  0.003 ,\n",
       "       -0.0736, -0.0815, -0.0826, -0.0204,  0.0729, -0.0184,  0.017 ,\n",
       "       -0.0696,  0.0688,  0.0667,  0.0261, -0.0011,  0.038 , -0.0294,\n",
       "       -0.0337,  0.0439, -0.0628,  0.0702,  0.0522,  0.0593, -0.0353,\n",
       "       -0.0072,  0.0105,  0.0025,  0.0458,  0.0251,  0.0166, -0.0179,\n",
       "       -0.0464,  0.0182,  0.004 ,  0.0663,  0.058 ,  0.0452, -0.07  ,\n",
       "        0.0413, -0.0312,  0.0109, -0.0122,  0.0202,  0.0221,  0.0444,\n",
       "       -0.0378, -0.0182,  0.0668, -0.036 ,  0.054 ,  0.0325,  0.0163,\n",
       "       -0.0124, -0.0576,  0.0455, -0.0298, -0.0358,  0.0764,  0.0411,\n",
       "       -0.0028, -0.0327, -0.0232,  0.0099, -0.0147, -0.0891],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"absolute\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
