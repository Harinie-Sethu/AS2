{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06977b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/kapilrk04/cache'\n",
    "os.environ['HF_DATASETS_CACHE']=\"/scratch/kapilrk04/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b90042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "wiki_qa_dataset = load_dataset(\"wiki_qa\")\n",
    "\n",
    "wiki_qa_dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_set = {\n",
    "    \"train\" : {},\n",
    "    \"validation\" : {},\n",
    "    \"test\" : {}\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for example in wiki_qa_dataset[split]:\n",
    "        if example[\"question_id\"] not in wiki_qa_set[split]:\n",
    "            wiki_qa_set[split][example[\"question_id\"]] = {\n",
    "                \"question\" : example[\"question\"],\n",
    "                \"answers\" : [],\n",
    "                \"labels\" : [],\n",
    "                \"sum_labels\" : 0\n",
    "            }\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"answers\"].append(example[\"answer\"])\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"labels\"].append(example[\"label\"])\n",
    "        wiki_qa_set[split][example[\"question_id\"]][\"sum_labels\"] += example[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b49de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_qa_trainp = [{\"sentence1\" : wiki_qa_set[\"train\"][qn][\"question\"], \"sentence2\" : wiki_qa_set[\"train\"][qn][\"answers\"][i], \"label\" : wiki_qa_set[\"train\"][qn][\"labels\"][i]} for qn in wiki_qa_set[\"train\"] for i in range(len(wiki_qa_set[\"train\"][qn][\"answers\"])) if wiki_qa_set[\"train\"][qn][\"sum_labels\"] > 0 and wiki_qa_set[\"train\"][qn][\"sum_labels\"] < len(wiki_qa_set[\"train\"][qn][\"labels\"])]\n",
    "wiki_qa_validationp = [{\"sentence1\" : wiki_qa_set[\"validation\"][qn][\"question\"], \"sentence2\" : wiki_qa_set[\"validation\"][qn][\"answers\"][i], \"label\" : wiki_qa_set[\"validation\"][qn][\"labels\"][i]} for qn in wiki_qa_set[\"validation\"] for i in range(len(wiki_qa_set[\"validation\"][qn][\"answers\"])) if wiki_qa_set[\"validation\"][qn][\"sum_labels\"] > 0 and wiki_qa_set[\"validation\"][qn][\"sum_labels\"] < len(wiki_qa_set[\"validation\"][qn][\"labels\"])]\n",
    "wiki_qa_testp = [{\"sentence1\" : wiki_qa_set[\"test\"][qn][\"question\"], \"sentence2\" : wiki_qa_set[\"test\"][qn][\"answers\"][i], \"label\" : wiki_qa_set[\"test\"][qn][\"labels\"][i]} for qn in wiki_qa_set[\"test\"] for i in range(len(wiki_qa_set[\"test\"][qn][\"answers\"])) if wiki_qa_set[\"test\"][qn][\"sum_labels\"] > 0 and wiki_qa_set[\"test\"][qn][\"sum_labels\"] < len(wiki_qa_set[\"test\"][qn][\"labels\"])]\n",
    "\n",
    "wiki_qa_trainp = pd.DataFrame(wiki_qa_trainp)\n",
    "wiki_qa_validationp = pd.DataFrame(wiki_qa_validationp)\n",
    "wiki_qa_testp = pd.DataFrame(wiki_qa_testp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_trainp['idx'] = range(1, len(wiki_qa_trainp)+1)\n",
    "wiki_qa_validationp['idx'] = range(1, len(wiki_qa_validationp)+1)\n",
    "wiki_qa_testp['idx'] = range(1, len(wiki_qa_testp)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_ds = Dataset.from_pandas(wiki_qa_trainp)\n",
    "wiki_test_ds = Dataset.from_pandas(wiki_qa_testp)\n",
    "wiki_valid_ds = Dataset.from_pandas(wiki_qa_validationp)\n",
    "\n",
    "print(len(wiki_train_ds), len(wiki_test_ds), len(wiki_valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35775651",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = {\n",
    "    \"distilbert-base-uncased\": \"/scratch/kapilrk04/best-distilbert/checkpoint-21468\",\n",
    "    \"roberta-base\": \"/scratch/kapilrk04/best-roberta/checkpoint-21468\",\n",
    "    \"bert-base-uncased\": \"/scratch/kapilrk04/best-bert/checkpoint-37569\",\n",
    "    \"albert-base-v2\": \"/scratch/kapilrk04/best-albert/checkpoint-16101\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "# encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "# encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "# encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('SEP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2914c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array_by_number(arr, number):\n",
    "    result = []\n",
    "    current_split = []\n",
    "    \n",
    "    for item in arr:\n",
    "        if item == number:\n",
    "            if current_split:\n",
    "                result.append(current_split)\n",
    "                return current_split\n",
    "        else:\n",
    "            current_split.append(item)\n",
    "    if current_split:\n",
    "        result.append(current_split)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels, inputs = eval_pred\n",
    "    \n",
    "    splitnum = 0\n",
    "    if model_name == \"roberta-base\":\n",
    "        splitnum = 2\n",
    "    elif model_name == \"bert-base-uncased\":\n",
    "        splitnum = 102\n",
    "    elif model_name == \"albert-base-v2\":\n",
    "        splitnum = 3\n",
    "    elif model_name == \"distilbert-base-uncased\":\n",
    "        splitnum = 102\n",
    "\n",
    "    per_qn_inputs = {}\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        split_inputs = split_array_by_number(inputs[i], splitnum)\n",
    "        qn = tuple(split_inputs)\n",
    "        if qn not in per_qn_inputs:\n",
    "            per_qn_inputs[qn] = {}\n",
    "            per_qn_inputs[qn][\"predictions\"] = []\n",
    "            per_qn_inputs[qn][\"labels\"] = []\n",
    "            per_qn_inputs[qn][\"sum_labels\"] = 0\n",
    "        per_qn_inputs[qn][\"predictions\"].append(predictions[i])\n",
    "        per_qn_inputs[qn][\"labels\"].append(labels[i])\n",
    "        per_qn_inputs[qn][\"sum_labels\"] += labels[i]\n",
    "\n",
    "    avg_prec_scores = []\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    labels = enc.fit_transform(np.array(labels).reshape(-1,1))\n",
    "\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for qn in per_qn_inputs:\n",
    "        if per_qn_inputs[qn][\"sum_labels\"] == 0 or per_qn_inputs[qn][\"sum_labels\"] == len(per_qn_inputs[qn][\"labels\"]):\n",
    "            continue\n",
    "        per_qn_inputs[qn]['predictions'] = np.array(per_qn_inputs[qn]['predictions'])\n",
    "        per_qn_inputs[qn]['labels'] = enc.fit_transform(np.array(per_qn_inputs[qn]['labels']).reshape(-1,1))\n",
    "\n",
    "        #print(per_qn_inputs[qn]['predictions'], per_qn_inputs[qn]['labels'])\n",
    "        avg_prec_scores.append(average_precision_score(per_qn_inputs[qn][\"labels\"], per_qn_inputs[qn][\"predictions\"]))\n",
    "\n",
    "        true_label = per_qn_inputs[qn][\"labels\"]\n",
    "        pred_label = per_qn_inputs[qn][\"predictions\"]\n",
    "\n",
    "        sorted_pred_label = np.argsort(pred_label)[::-1]\n",
    "\n",
    "        for j in range(len(sorted_pred_label)):\n",
    "            row = sorted_pred_label[j]\n",
    "            rank = np.where(row == 1)[0]\n",
    "            if rank.size > 0:\n",
    "                reciprocal_ranks.append(1/(rank[0]+1))\n",
    "                break\n",
    "    \n",
    "    \n",
    "    map_score = np.mean(avg_prec_scores)\n",
    "    mrr_score = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    print(\"mAP: \", map_score)\n",
    "    print(\"mRR: \", mrr_score)\n",
    "    return {\n",
    "        \"mAP\" : map_score,\n",
    "        \"mRR\" : mrr_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c087559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2725091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting model params\n",
    "for model_name in [\"bert-base-uncased\", 'roberta-base', 'distilbert-base-uncased', 'albert-base-v2']:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of model parameters in {model_name}: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680a0b8",
   "metadata": {},
   "source": [
    "### Check stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs):\n",
    "    batch_size = 8\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "    encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "    encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "    args1 = TrainingArguments(\n",
    "        output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_wikiqa\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-6,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        include_inputs_for_metrics = True,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"tanda-{model_name}-eval-wikiqa\"\n",
    "    )\n",
    "    \n",
    "    args2 = TrainingArguments(\n",
    "        output_dir=f\"/scratch/kapilrk04/{model_name}_ft_model_wikiqa\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-6,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        include_inputs_for_metrics = True,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"ft-{model_name}-eval-wikiqa\"\n",
    "    )\n",
    "\n",
    "    trainer1 = Trainer(\n",
    "        model,\n",
    "        args1,\n",
    "        train_dataset=encoded_train_dataset,\n",
    "        eval_dataset=encoded_dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer11 = Trainer(\n",
    "        base_model,\n",
    "        args2,\n",
    "        train_dataset=encoded_train_dataset,\n",
    "        eval_dataset=encoded_dev_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer2 = Trainer(\n",
    "        model,\n",
    "        args1,\n",
    "        train_dataset=encoded_train_dataset,\n",
    "        eval_dataset=encoded_test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer21 = Trainer(\n",
    "        base_model,\n",
    "        args2,\n",
    "        train_dataset=encoded_train_dataset,\n",
    "        eval_dataset=encoded_test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"BERT FT\")\n",
    "    trainer11.train()\n",
    "    trainer21.evaluate()\n",
    "    \n",
    "    print(\"TANDA\")\n",
    "    trainer1.train()\n",
    "    trainer2.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b3767",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 7):\n",
    "    print(\"Epoch \", epoch)\n",
    "    train_loop(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_maps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2c7c1",
   "metadata": {},
   "source": [
    "## For WikiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232609d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-wikiqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de50855",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa56bf",
   "metadata": {},
   "source": [
    "### DISTILBERT\n",
    "\n",
    "#### Evaluation without Adapt Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dc8a9",
   "metadata": {},
   "source": [
    "### Adapt Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ae877",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24925256",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbebd10",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-wikiqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032347cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265b713",
   "metadata": {},
   "source": [
    "### Adapt Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b503c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afbdbc",
   "metadata": {},
   "source": [
    "### BERT-base\n",
    "\n",
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-wikiqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e78fdd",
   "metadata": {},
   "source": [
    "#### Evaluation without adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d099a",
   "metadata": {},
   "source": [
    "### Adapt Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa9ea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1d0b4",
   "metadata": {},
   "source": [
    "### ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wiki_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wiki_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wiki_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-wikiqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250d229",
   "metadata": {},
   "source": [
    "#### Evaluation without adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041805b",
   "metadata": {},
   "source": [
    "#### ADAPT then eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdf661",
   "metadata": {},
   "source": [
    "## For TrecQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc63e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trecqa_train = pd.read_csv(\"/home2/kapilrk04/anlp_proj/data_sets/TrecQA/train.tsv\", sep=\"\\t\", names=[\"label\", \"sentence1\", \"sentence2\"])\n",
    "trecqa_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trecqa_test = pd.read_csv(\"/home2/kapilrk04/anlp_proj/data_sets/TrecQA/test.tsv\", sep=\"\\t\", names=[\"label\", \"sentence1\", \"sentence2\"])\n",
    "trecqa_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ce75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trecqa_dev = pd.read_csv(\"/home2/kapilrk04/anlp_proj/data_sets/TrecQA/dev.tsv\", sep=\"\\t\", names=[\"label\", \"sentence1\", \"sentence2\"])\n",
    "trecqa_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trecqa_train['idx'] = range(1, len(trecqa_train)+1)\n",
    "trecqa_dev['idx'] = range(1, len(trecqa_dev)+1)\n",
    "trecqa_test['idx'] = range(1, len(trecqa_test)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8529c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(trecqa_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(trecqa_dev['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(trecqa_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trecqa_train_ds = Dataset.from_pandas(trecqa_train)\n",
    "trecqa_test_ds = Dataset.from_pandas(trecqa_test)\n",
    "trecqa_valid_ds = Dataset.from_pandas(trecqa_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_trecqa_train_ds = trecqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_test_ds = trecqa_test_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_valid_ds = trecqa_valid_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c1833",
   "metadata": {},
   "source": [
    "### DISTILBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a240a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_trecqa_train_ds = trecqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_test_ds = trecqa_test_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_valid_ds = trecqa_valid_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_trecqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-trecqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de691f08",
   "metadata": {},
   "source": [
    "#### Eval without Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed90c2",
   "metadata": {},
   "source": [
    "#### Adapt and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ed9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffef2b",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7651a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_trecqa_train_ds = trecqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_test_ds = trecqa_test_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_valid_ds = trecqa_valid_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_trecqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-trecqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f4f68",
   "metadata": {},
   "source": [
    "#### evaluate without adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b75cf",
   "metadata": {},
   "source": [
    "#### adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659149f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99527aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943ca3a",
   "metadata": {},
   "source": [
    "### BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_trecqa_train_ds = trecqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_test_ds = trecqa_test_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_valid_ds = trecqa_valid_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_trecqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-trecqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8461d",
   "metadata": {},
   "source": [
    "#### evaluate without adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f8b2a",
   "metadata": {},
   "source": [
    "#### adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428595e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7146d6",
   "metadata": {},
   "source": [
    "### ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be880f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "\n",
    "encoded_trecqa_train_ds = trecqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_test_ds = trecqa_test_ds.map(preprocess_function, batched=True)\n",
    "encoded_trecqa_valid_ds = trecqa_valid_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_trecqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-trecqa\"\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model,\n",
    "    args1,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_trecqa_train_ds,\n",
    "    eval_dataset=encoded_trecqa_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d169c",
   "metadata": {},
   "source": [
    "#### evaluate without adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8daa6f",
   "metadata": {},
   "source": [
    "#### adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ab0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ecf34",
   "metadata": {},
   "source": [
    "### TESTING MODEL ROBUST-NESS\n",
    "\n",
    "- By incorrectly labeling a portion of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inject_random_noise(df, noise_level=0.2):\n",
    "    noisy_df = df.copy()\n",
    "    num_samples_to_swap = int(len(noisy_df) * noise_level)\n",
    "    swap_indices = random.sample(range(len(noisy_df)), num_samples_to_swap)\n",
    "\n",
    "    for index in swap_indices:\n",
    "        row = noisy_df.iloc[index]\n",
    "        if row['label'] == 1:\n",
    "            noisy_df.at[index, 'label'] = 0\n",
    "        else:\n",
    "            noisy_df.at[index, 'label'] = 1\n",
    "    \n",
    "    return noisy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5fab8",
   "metadata": {},
   "source": [
    "#### WikiQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1912461",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_wiki_qa_trainp = inject_random_noise(wiki_qa_trainp, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2428dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa_train_ds = Dataset.from_pandas(noisy_wiki_qa_trainp)\n",
    "wikiqa_test_ds = Dataset.from_pandas(wiki_qa_testp)\n",
    "wikiqa_valid_ds = Dataset.from_pandas(wiki_qa_validationp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a120644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wikiqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wikiqa_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wikiqa_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_base-model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-base-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer11 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=base_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer12 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=base_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer21 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer22 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16dbc7e",
   "metadata": {},
   "source": [
    "#### baseline: adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer11.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer12.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c364c",
   "metadata": {},
   "source": [
    "#### transferred model - adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer21.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9010a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer22.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39083950",
   "metadata": {},
   "source": [
    "#### noise 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3330b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_wiki_qa_trainp = inject_random_noise(wiki_qa_trainp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a71d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(noisy_wiki_qa_trainp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wiki_qa_validationp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wiki_qa_testp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3282c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa_train_ds = Dataset.from_pandas(noisy_wiki_qa_trainp)\n",
    "wikiqa_test_ds = Dataset.from_pandas(wiki_qa_testp)\n",
    "wikiqa_valid_ds = Dataset.from_pandas(wiki_qa_validationp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddba43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wikiqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wikiqa_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wikiqa_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_base-model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-base-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer11 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer12 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer21 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer22 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdefb3",
   "metadata": {},
   "source": [
    "#### baseline: adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer11.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer12.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33869f3c",
   "metadata": {},
   "source": [
    "#### transferred model - adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525eeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer21.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer22.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19ff50",
   "metadata": {},
   "source": [
    "#### no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e02dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wiki_qa_trainp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wiki_qa_validationp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wiki_qa_testp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe98776",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa_train_ds = Dataset.from_pandas(noisy_wiki_qa_trainp)\n",
    "wikiqa_test_ds = Dataset.from_pandas(wiki_qa_testp)\n",
    "wikiqa_valid_ds = Dataset.from_pandas(wiki_qa_validationp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints[model_name], use_fast=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoints[model_name], num_labels=2)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "encoded_train_dataset = wikiqa_train_ds.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = wikiqa_valid_ds.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = wikiqa_test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=f\"/scratch/kapilrk04/{model_name}_adapt_base-model_noisy_wikiqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    include_inputs_for_metrics = True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"tanda-base-{model_name}-eval-noisy-wikiqa\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer11 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer12 = Trainer(\n",
    "    base_model,\n",
    "    args1,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer21 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer22 = Trainer(\n",
    "    model,\n",
    "    args2,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1622b3",
   "metadata": {},
   "source": [
    "#### baseline: adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aac148",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer11.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd30686",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer12.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a49292",
   "metadata": {},
   "source": [
    "#### transferred model - adapt then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer21.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11085d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer22.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
