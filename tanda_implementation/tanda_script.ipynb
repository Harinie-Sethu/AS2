{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6529dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b8ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/kapilrk04/cache'\n",
    "os.environ['HF_DATASETS_CACHE']=\"/scratch/kapilrk04/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0356abfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d09aa472a4526bf4aaed42af16c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25b7c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d88071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: sentencepiece in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (0.1.99)\n",
      "Requirement already satisfied: filelock in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: six in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5094b628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from accelerate) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: filelock in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2023.4.0)\n",
      "Requirement already satisfied: requests in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b069d56b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (2023.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: six in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from responses<0.19->evaluate) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036717b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home2/kapilrk04/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f263e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "asnq_dev = pd.read_csv(\"/home2/kapilrk04/anlp_proj/data_sets/asnq/dev.tsv\", sep=\"\\t\", names=[\"sentence1\", \"sentence2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b20b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who issued gold coins for the first time in india</td>\n",
       "      <td>Jump up ^ `` Puranas or Punch - Marked Coins (...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>most passing yards by nfl qb in a game</td>\n",
       "      <td>71 : Kramer , Tommy Tommy Kramer 000000001985 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>where does cortisol come from in the body</td>\n",
       "      <td>Psychoneuroendocrinology.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the rime of the ancient mariner albatross symb...</td>\n",
       "      <td>Contents ( hide ) 1 Film 2 Music 2.1 Musical 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who was it that described the structure of dna</td>\n",
       "      <td>If the DNA is twisted in the direction of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>who dies in the beginning of deathly hallows p...</td>\n",
       "      <td>Retrieved 20 March 2010 .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what is cost of first class mail stamp</td>\n",
       "      <td>Main article : United States Postal Service Se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when did somewhere over the rainbow come out</td>\n",
       "      <td>In films and TV shows ( edit ) In the film Thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>where did they film the game of thrones</td>\n",
       "      <td>The fifth season was also well received by cri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what observation led to the theory of seafloor...</td>\n",
       "      <td>Older seafloor is therefore colder than new se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>where did students for a democratic society start</td>\n",
       "      <td>The media began to cover the organization and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>is the united states a country or nation</td>\n",
       "      <td>Jump up ^ `` Louisiana Purchase '' ( PDF ) .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>how many ounces in one shot of liquor</td>\n",
       "      <td>A contemporary jigger measure in the U.S. usua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what was the real name of saudi arabia</td>\n",
       "      <td>Retrieved 16 September 2015 .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>india gdp growth rate in last 10 years</td>\n",
       "      <td>India 's income Gini coefficient is 33.9 , acc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence1  \\\n",
       "0   who issued gold coins for the first time in india   \n",
       "1              most passing yards by nfl qb in a game   \n",
       "2           where does cortisol come from in the body   \n",
       "3   the rime of the ancient mariner albatross symb...   \n",
       "4      who was it that described the structure of dna   \n",
       "5   who dies in the beginning of deathly hallows p...   \n",
       "6              what is cost of first class mail stamp   \n",
       "7        when did somewhere over the rainbow come out   \n",
       "8             where did they film the game of thrones   \n",
       "9   what observation led to the theory of seafloor...   \n",
       "10  where did students for a democratic society start   \n",
       "11           is the united states a country or nation   \n",
       "12              how many ounces in one shot of liquor   \n",
       "13             what was the real name of saudi arabia   \n",
       "14             india gdp growth rate in last 10 years   \n",
       "\n",
       "                                            sentence2  label  \n",
       "0   Jump up ^ `` Puranas or Punch - Marked Coins (...      1  \n",
       "1   71 : Kramer , Tommy Tommy Kramer 000000001985 ...      3  \n",
       "2                           Psychoneuroendocrinology.      1  \n",
       "3   Contents ( hide ) 1 Film 2 Music 2.1 Musical 2...      1  \n",
       "4   If the DNA is twisted in the direction of the ...      1  \n",
       "5                           Retrieved 20 March 2010 .      1  \n",
       "6   Main article : United States Postal Service Se...      1  \n",
       "7   In films and TV shows ( edit ) In the film Thi...      1  \n",
       "8   The fifth season was also well received by cri...      1  \n",
       "9   Older seafloor is therefore colder than new se...      1  \n",
       "10  The media began to cover the organization and ...      1  \n",
       "11       Jump up ^ `` Louisiana Purchase '' ( PDF ) .      1  \n",
       "12  A contemporary jigger measure in the U.S. usua...      1  \n",
       "13                      Retrieved 16 September 2015 .      1  \n",
       "14  India 's income Gini coefficient is 33.9 , acc...      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asnq_dev[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07e16f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    870404\n",
       "3     29558\n",
       "2     25814\n",
       "4      4286\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(asnq_dev[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfee340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asnq_train = pd.read_csv(\"/home2/kapilrk04/anlp_proj/data_sets/asnq/train.tsv\", sep=\"\\t\", names=[\"sentence1\", \"sentence2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e61716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the use of fn key in mac</td>\n",
       "      <td>It is typically found on laptops due to their ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when did the ipod 7th generation come out</td>\n",
       "      <td>Cupertino , California : Apple .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who dies in season 6 once upon a time</td>\n",
       "      <td>^ Jump up to : Abrams , Natalie ( August 5 , 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who was the first avatar ever in the last airb...</td>\n",
       "      <td>Jeremy Zuckerman and Benjamin Wynn composed th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many games do they need to win to win the ...</td>\n",
       "      <td>p. 21 .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>when did rufus wainwright record across the un...</td>\n",
       "      <td>266 -- 67 Jump up ^ Sheff , p. 266 Jump up ^ K...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>two core functions of the world health organiz...</td>\n",
       "      <td>In 1977 , the first list of essential medicine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when did adam smith published the wealth of na...</td>\n",
       "      <td>Boston Review .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>how many english soldiers died in world war 2</td>\n",
       "      <td>Statistics published in Russia list civilian w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is the location of gateway of india</td>\n",
       "      <td>Wikipedia ® is a registered trademark of the W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>who designed the holocaust museum in washingto...</td>\n",
       "      <td>Managing heritage and cultural tourism resourc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>who is the main character in dishonored 2</td>\n",
       "      <td>Archived from the original on 12 June 2017 .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>when is the next episode of dragon ball super ...</td>\n",
       "      <td>18 and Frieza each defeat Sorrel ( ソレル , Sorer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>where does the american english language come ...</td>\n",
       "      <td>Accents of English : Vowel 3 : Beyond the Brit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for which group did michael jackson perform as...</td>\n",
       "      <td>13 .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence1  \\\n",
       "0                    what is the use of fn key in mac   \n",
       "1           when did the ipod 7th generation come out   \n",
       "2               who dies in season 6 once upon a time   \n",
       "3   who was the first avatar ever in the last airb...   \n",
       "4   how many games do they need to win to win the ...   \n",
       "5   when did rufus wainwright record across the un...   \n",
       "6   two core functions of the world health organiz...   \n",
       "7   when did adam smith published the wealth of na...   \n",
       "8       how many english soldiers died in world war 2   \n",
       "9            what is the location of gateway of india   \n",
       "10  who designed the holocaust museum in washingto...   \n",
       "11          who is the main character in dishonored 2   \n",
       "12  when is the next episode of dragon ball super ...   \n",
       "13  where does the american english language come ...   \n",
       "14  for which group did michael jackson perform as...   \n",
       "\n",
       "                                            sentence2  label  \n",
       "0   It is typically found on laptops due to their ...      1  \n",
       "1                    Cupertino , California : Apple .      1  \n",
       "2   ^ Jump up to : Abrams , Natalie ( August 5 , 2...      1  \n",
       "3   Jeremy Zuckerman and Benjamin Wynn composed th...      1  \n",
       "4                                             p. 21 .      1  \n",
       "5   266 -- 67 Jump up ^ Sheff , p. 266 Jump up ^ K...      1  \n",
       "6   In 1977 , the first list of essential medicine...      1  \n",
       "7                                     Boston Review .      1  \n",
       "8   Statistics published in Russia list civilian w...      1  \n",
       "9   Wikipedia ® is a registered trademark of the W...      1  \n",
       "10  Managing heritage and cultural tourism resourc...      1  \n",
       "11       Archived from the original on 12 June 2017 .      1  \n",
       "12  18 and Frieza each defeat Sorrel ( ソレル , Sorer...      1  \n",
       "13  Accents of English : Vowel 3 : Beyond the Brit...      1  \n",
       "14                                               13 .      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asnq_train[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d4eab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    19446120\n",
       "3      442140\n",
       "2      428122\n",
       "4       61186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(asnq_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37b86d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNeg = asnq_train[asnq_train['label']==3].sample(frac=0.25)\n",
    "trainNeg.loc[:,'label'] = 0\n",
    "trainPos = asnq_train[asnq_train['label']==4]\n",
    "trainPos.loc[:,'label'] = 1\n",
    "\n",
    "train_set = pd.concat([trainNeg, trainPos])\n",
    "train_set['idx'] = range(1, len(train_set) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a103f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14043360</th>\n",
       "      <td>where is the big 10 basketball tournament held</td>\n",
       "      <td>Due to the Big East 's use of that venue for t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504059</th>\n",
       "      <td>explain the difference between recorded and li...</td>\n",
       "      <td>Within the arts , music may be classified as a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160749</th>\n",
       "      <td>who won the women's 2017 us open</td>\n",
       "      <td>It was her first major and first LPGA Tour win...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738895</th>\n",
       "      <td>when do you get your unrestricted license in m...</td>\n",
       "      <td>As some of the 80000 decals sold are for drive...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696439</th>\n",
       "      <td>who has the most blocks in a nba game</td>\n",
       "      <td>13 Johnson , George T. George T. Johnson San A...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sentence1  \\\n",
       "14043360     where is the big 10 basketball tournament held   \n",
       "9504059   explain the difference between recorded and li...   \n",
       "7160749                    who won the women's 2017 us open   \n",
       "5738895   when do you get your unrestricted license in m...   \n",
       "9696439               who has the most blocks in a nba game   \n",
       "\n",
       "                                                  sentence2  label  idx  \n",
       "14043360  Due to the Big East 's use of that venue for t...      0    1  \n",
       "9504059   Within the arts , music may be classified as a...      0    2  \n",
       "7160749   It was her first major and first LPGA Tour win...      0    3  \n",
       "5738895   As some of the 80000 decals sold are for drive...      0    4  \n",
       "9696439   13 Johnson , George T. George T. Johnson San A...      0    5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d1c5b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    110535\n",
       "1     61186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(train_set[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "devNeg = asnq_dev[asnq_dev['label']==3].sample(frac=0.25)\n",
    "devNeg.loc[:,'label'] = 0\n",
    "devPos = asnq_dev[asnq_dev['label']==4]\n",
    "devPos.loc[:,'label'] = 1\n",
    "\n",
    "dev_set = pd.concat([devNeg, devPos])\n",
    "dev_set['idx'] = range(1, len(dev_set) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb3c70c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>538496</th>\n",
       "      <td>where does no game no life anime end</td>\n",
       "      <td>focusing on the character Izuna , ran from May...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110546</th>\n",
       "      <td>what is the longest panic at the disco song title</td>\n",
       "      <td>Alternate version featured on deluxe version o...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9637</th>\n",
       "      <td>who wrote trust and believe by keyshia cole</td>\n",
       "      <td>The heartfelt ballad reflects upon Cole 's pos...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623470</th>\n",
       "      <td>when were the beatles inducted into the rock a...</td>\n",
       "      <td>Four Tops Renaldo `` Obie '' Benson , Abdul ``...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525264</th>\n",
       "      <td>what type of economic system was utilized in t...</td>\n",
       "      <td>In practice this limited the private sector to...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "538496               where does no game no life anime end   \n",
       "110546  what is the longest panic at the disco song title   \n",
       "9637          who wrote trust and believe by keyshia cole   \n",
       "623470  when were the beatles inducted into the rock a...   \n",
       "525264  what type of economic system was utilized in t...   \n",
       "\n",
       "                                                sentence2  label  idx  \n",
       "538496  focusing on the character Izuna , ran from May...      0    1  \n",
       "110546  Alternate version featured on deluxe version o...      0    2  \n",
       "9637    The heartfelt ballad reflects upon Cole 's pos...      0    3  \n",
       "623470  Four Tops Renaldo `` Obie '' Benson , Abdul ``...      0    4  \n",
       "525264  In practice this limited the private sector to...      0    5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86e3514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    7390\n",
       "1    4286\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(dev_set[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a019c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_set)\n",
    "dev_dataset = Dataset.from_pandas(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d2f50c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 171721\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e81ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns('__index_level_0__')\n",
    "dev_dataset = dev_dataset.remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d19c92a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 11676\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d901661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52829a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_dev_dataset = dev_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92cb40b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 11676\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1408763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['where is the big 10 basketball tournament held',\n",
       "  'explain the difference between recorded and live music',\n",
       "  \"who won the women's 2017 us open\",\n",
       "  'when do you get your unrestricted license in michigan',\n",
       "  'who has the most blocks in a nba game'],\n",
       " 'sentence2': [\"Due to the Big East 's use of that venue for their conference tournament , the Big Ten Tournament took place one week earlier than usual , ending the week before Selection Sunday .\",\n",
       "  'Within the arts , music may be classified as a performing art , a fine art or as an auditory art .',\n",
       "  'It was her first major and first LPGA Tour win after 10 wins on the LPGA of Korea Tour .',\n",
       "  'As some of the 80000 decals sold are for drivers with multiple cars , it is estimated that 75 % of provisional drivers ignore this law .',\n",
       "  '13 Johnson , George T. George T. Johnson San Antonio Spurs 000000001981 - 02 - 24 - 0000 February 24 , 1981 Golden State Warriors 131 -- 126 28 9 9 0 No Yes 11 blocks in the second half , tying the NBA record .'],\n",
       " 'label': [0, 0, 0, 0, 0],\n",
       " 'idx': [1, 2, 3, 4, 5],\n",
       " 'input_ids': [[0,\n",
       "   8569,\n",
       "   16,\n",
       "   5,\n",
       "   380,\n",
       "   158,\n",
       "   2613,\n",
       "   1967,\n",
       "   547,\n",
       "   2,\n",
       "   2,\n",
       "   28084,\n",
       "   7,\n",
       "   5,\n",
       "   1776,\n",
       "   953,\n",
       "   128,\n",
       "   29,\n",
       "   304,\n",
       "   9,\n",
       "   14,\n",
       "   5584,\n",
       "   13,\n",
       "   49,\n",
       "   1019,\n",
       "   1967,\n",
       "   2156,\n",
       "   5,\n",
       "   1776,\n",
       "   4527,\n",
       "   7647,\n",
       "   362,\n",
       "   317,\n",
       "   65,\n",
       "   186,\n",
       "   656,\n",
       "   87,\n",
       "   4505,\n",
       "   2156,\n",
       "   3558,\n",
       "   5,\n",
       "   186,\n",
       "   137,\n",
       "   30418,\n",
       "   395,\n",
       "   479,\n",
       "   2],\n",
       "  [0,\n",
       "   23242,\n",
       "   1851,\n",
       "   5,\n",
       "   2249,\n",
       "   227,\n",
       "   2673,\n",
       "   8,\n",
       "   697,\n",
       "   930,\n",
       "   2,\n",
       "   2,\n",
       "   35469,\n",
       "   5,\n",
       "   5444,\n",
       "   2156,\n",
       "   930,\n",
       "   189,\n",
       "   28,\n",
       "   8967,\n",
       "   25,\n",
       "   10,\n",
       "   4655,\n",
       "   1808,\n",
       "   2156,\n",
       "   10,\n",
       "   2051,\n",
       "   1808,\n",
       "   50,\n",
       "   25,\n",
       "   41,\n",
       "   44630,\n",
       "   1808,\n",
       "   479,\n",
       "   2],\n",
       "  [0,\n",
       "   8155,\n",
       "   351,\n",
       "   5,\n",
       "   390,\n",
       "   18,\n",
       "   193,\n",
       "   201,\n",
       "   490,\n",
       "   2,\n",
       "   2,\n",
       "   243,\n",
       "   21,\n",
       "   69,\n",
       "   78,\n",
       "   538,\n",
       "   8,\n",
       "   78,\n",
       "   226,\n",
       "   8332,\n",
       "   250,\n",
       "   3637,\n",
       "   339,\n",
       "   71,\n",
       "   158,\n",
       "   2693,\n",
       "   15,\n",
       "   5,\n",
       "   226,\n",
       "   8332,\n",
       "   250,\n",
       "   9,\n",
       "   1101,\n",
       "   3637,\n",
       "   479,\n",
       "   2],\n",
       "  [0,\n",
       "   14746,\n",
       "   109,\n",
       "   47,\n",
       "   120,\n",
       "   110,\n",
       "   28225,\n",
       "   4385,\n",
       "   11,\n",
       "   475,\n",
       "   1725,\n",
       "   6703,\n",
       "   2,\n",
       "   2,\n",
       "   1620,\n",
       "   103,\n",
       "   9,\n",
       "   5,\n",
       "   290,\n",
       "   14200,\n",
       "   5044,\n",
       "   1536,\n",
       "   1088,\n",
       "   32,\n",
       "   13,\n",
       "   2377,\n",
       "   19,\n",
       "   1533,\n",
       "   1677,\n",
       "   2156,\n",
       "   24,\n",
       "   16,\n",
       "   2319,\n",
       "   14,\n",
       "   3337,\n",
       "   7606,\n",
       "   9,\n",
       "   21571,\n",
       "   2377,\n",
       "   8861,\n",
       "   42,\n",
       "   488,\n",
       "   479,\n",
       "   2],\n",
       "  [0,\n",
       "   8155,\n",
       "   34,\n",
       "   5,\n",
       "   144,\n",
       "   5491,\n",
       "   11,\n",
       "   10,\n",
       "   295,\n",
       "   3178,\n",
       "   177,\n",
       "   2,\n",
       "   2,\n",
       "   1558,\n",
       "   1436,\n",
       "   2156,\n",
       "   1655,\n",
       "   255,\n",
       "   4,\n",
       "   1655,\n",
       "   255,\n",
       "   4,\n",
       "   1436,\n",
       "   764,\n",
       "   4578,\n",
       "   7963,\n",
       "   49524,\n",
       "   44582,\n",
       "   111,\n",
       "   15140,\n",
       "   111,\n",
       "   706,\n",
       "   111,\n",
       "   47489,\n",
       "   902,\n",
       "   706,\n",
       "   2156,\n",
       "   14130,\n",
       "   3274,\n",
       "   331,\n",
       "   4223,\n",
       "   22236,\n",
       "   480,\n",
       "   18461,\n",
       "   971,\n",
       "   361,\n",
       "   361,\n",
       "   321,\n",
       "   440,\n",
       "   3216,\n",
       "   365,\n",
       "   5491,\n",
       "   11,\n",
       "   5,\n",
       "   200,\n",
       "   457,\n",
       "   2156,\n",
       "   13024,\n",
       "   5,\n",
       "   2762,\n",
       "   638,\n",
       "   479,\n",
       "   2]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0eefca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1d37ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    avg_prec_scores = []\n",
    "    num_queries = len(labels)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        avg_prec_scores.append(average_precision_score(labels[i], predictions[i]))\n",
    "    \n",
    "    map_score = np.mean(avg_prec_scores)\n",
    "\n",
    "    mrr_sum = 0\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        mrr_sum += 1/(np.where(labels[i]==1)[0][0]+1)\n",
    "    \n",
    "    mrr_score = mrr_sum/num_queries\n",
    "\n",
    "    return {\n",
    "        \"mAP\" : map_score,\n",
    "        \"MRR\" : mrr_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee0040c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['where is the big 10 basketball tournament held',\n",
       "  'explain the difference between recorded and live music',\n",
       "  \"who won the women's 2017 us open\",\n",
       "  'when do you get your unrestricted license in michigan',\n",
       "  'who has the most blocks in a nba game',\n",
       "  \"who was the first singing brothers inducted into the rock 'n' roll hall of fame\",\n",
       "  'where did the idea of spongebob come from',\n",
       "  'who did snow white fall in love with',\n",
       "  'when did the first motorola flip phone come out',\n",
       "  'when does jane the virgin season 4 episode 13 come out',\n",
       "  'when was attack on titan season 1 released',\n",
       "  'who wrote never be alone by shawn mendes',\n",
       "  'who has the most figure skating olympic medals',\n",
       "  'list of chapters in a book is called',\n",
       "  'when does jake show up in hannah montana'],\n",
       " 'sentence2': [\"Due to the Big East 's use of that venue for their conference tournament , the Big Ten Tournament took place one week earlier than usual , ending the week before Selection Sunday .\",\n",
       "  'Within the arts , music may be classified as a performing art , a fine art or as an auditory art .',\n",
       "  'It was her first major and first LPGA Tour win after 10 wins on the LPGA of Korea Tour .',\n",
       "  'As some of the 80000 decals sold are for drivers with multiple cars , it is estimated that 75 % of provisional drivers ignore this law .',\n",
       "  '13 Johnson , George T. George T. Johnson San Antonio Spurs 000000001981 - 02 - 24 - 0000 February 24 , 1981 Golden State Warriors 131 -- 126 28 9 9 0 No Yes 11 blocks in the second half , tying the NBA record .',\n",
       "  '2014 Ronstadt , Linda Linda Ronstadt 2014 Stevens , Cat Cat Stevens 2015 Butterfield Blues Band , The Paul The Paul Butterfield Blues Band Paul Butterfield , Mike Bloomfield , Elvin Bishop , Mark Naftalin , Jerome Arnold , Billy Davenport , and Sam Lay .',\n",
       "  'but both of these were changed , as the name was already trademarked .',\n",
       "  'The dwarfs return to their cottage and find Snow White seemingly dead , being kept in a deathlike slumber by the poison .',\n",
       "  'The Motorola StarTAC is a clamshell mobile phone manufactured by Motorola .',\n",
       "  'Rafael moves into the Villanueva house , but after Alba hits Mateo , the two get into a heated argument and Rafael moves back out .',\n",
       "  \"After Armin 's squad manages to kill an Abnormal Titan , a Titan with a feminine body suddenly appears from the right flank at high speed .\",\n",
       "  \"`` Kid in Love '' Mendes Zmishlany Harris Zmishlany Terefe 3 : 46 5 .\",\n",
       "  'Seventeen figure skaters have won three medals .',\n",
       "  'In modern books usually on the verso of the title page , but in some books placed at the end ( see Back matter ) .',\n",
       "  \"The queen is late for Hannah 's performance , which stalls the tournament .\"],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'idx': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       " 'input_ids': [[101,\n",
       "   2073,\n",
       "   2003,\n",
       "   1996,\n",
       "   2502,\n",
       "   2184,\n",
       "   3455,\n",
       "   2977,\n",
       "   2218,\n",
       "   102,\n",
       "   2349,\n",
       "   2000,\n",
       "   1996,\n",
       "   2502,\n",
       "   2264,\n",
       "   1005,\n",
       "   1055,\n",
       "   2224,\n",
       "   1997,\n",
       "   2008,\n",
       "   6891,\n",
       "   2005,\n",
       "   2037,\n",
       "   3034,\n",
       "   2977,\n",
       "   1010,\n",
       "   1996,\n",
       "   2502,\n",
       "   2702,\n",
       "   2977,\n",
       "   2165,\n",
       "   2173,\n",
       "   2028,\n",
       "   2733,\n",
       "   3041,\n",
       "   2084,\n",
       "   5156,\n",
       "   1010,\n",
       "   4566,\n",
       "   1996,\n",
       "   2733,\n",
       "   2077,\n",
       "   4989,\n",
       "   4465,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   4863,\n",
       "   1996,\n",
       "   4489,\n",
       "   2090,\n",
       "   2680,\n",
       "   1998,\n",
       "   2444,\n",
       "   2189,\n",
       "   102,\n",
       "   2306,\n",
       "   1996,\n",
       "   2840,\n",
       "   1010,\n",
       "   2189,\n",
       "   2089,\n",
       "   2022,\n",
       "   6219,\n",
       "   2004,\n",
       "   1037,\n",
       "   4488,\n",
       "   2396,\n",
       "   1010,\n",
       "   1037,\n",
       "   2986,\n",
       "   2396,\n",
       "   2030,\n",
       "   2004,\n",
       "   2019,\n",
       "   28042,\n",
       "   2396,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2180,\n",
       "   1996,\n",
       "   2308,\n",
       "   1005,\n",
       "   1055,\n",
       "   2418,\n",
       "   2149,\n",
       "   2330,\n",
       "   102,\n",
       "   2009,\n",
       "   2001,\n",
       "   2014,\n",
       "   2034,\n",
       "   2350,\n",
       "   1998,\n",
       "   2034,\n",
       "   6948,\n",
       "   3654,\n",
       "   2778,\n",
       "   2663,\n",
       "   2044,\n",
       "   2184,\n",
       "   5222,\n",
       "   2006,\n",
       "   1996,\n",
       "   6948,\n",
       "   3654,\n",
       "   1997,\n",
       "   4420,\n",
       "   2778,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2043,\n",
       "   2079,\n",
       "   2017,\n",
       "   2131,\n",
       "   2115,\n",
       "   16591,\n",
       "   20623,\n",
       "   6105,\n",
       "   1999,\n",
       "   4174,\n",
       "   102,\n",
       "   2004,\n",
       "   2070,\n",
       "   1997,\n",
       "   1996,\n",
       "   5385,\n",
       "   8889,\n",
       "   11703,\n",
       "   9777,\n",
       "   2853,\n",
       "   2024,\n",
       "   2005,\n",
       "   6853,\n",
       "   2007,\n",
       "   3674,\n",
       "   3765,\n",
       "   1010,\n",
       "   2009,\n",
       "   2003,\n",
       "   4358,\n",
       "   2008,\n",
       "   4293,\n",
       "   1003,\n",
       "   1997,\n",
       "   10864,\n",
       "   6853,\n",
       "   8568,\n",
       "   2023,\n",
       "   2375,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2038,\n",
       "   1996,\n",
       "   2087,\n",
       "   5991,\n",
       "   1999,\n",
       "   1037,\n",
       "   6452,\n",
       "   2208,\n",
       "   102,\n",
       "   2410,\n",
       "   3779,\n",
       "   1010,\n",
       "   2577,\n",
       "   1056,\n",
       "   1012,\n",
       "   2577,\n",
       "   1056,\n",
       "   1012,\n",
       "   3779,\n",
       "   2624,\n",
       "   4980,\n",
       "   18205,\n",
       "   2199,\n",
       "   8889,\n",
       "   8889,\n",
       "   24096,\n",
       "   2683,\n",
       "   2620,\n",
       "   2487,\n",
       "   1011,\n",
       "   6185,\n",
       "   1011,\n",
       "   2484,\n",
       "   1011,\n",
       "   2199,\n",
       "   2692,\n",
       "   2337,\n",
       "   2484,\n",
       "   1010,\n",
       "   3261,\n",
       "   3585,\n",
       "   2110,\n",
       "   6424,\n",
       "   14677,\n",
       "   1011,\n",
       "   1011,\n",
       "   14010,\n",
       "   2654,\n",
       "   1023,\n",
       "   1023,\n",
       "   1014,\n",
       "   2053,\n",
       "   2748,\n",
       "   2340,\n",
       "   5991,\n",
       "   1999,\n",
       "   1996,\n",
       "   2117,\n",
       "   2431,\n",
       "   1010,\n",
       "   15233,\n",
       "   1996,\n",
       "   6452,\n",
       "   2501,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2001,\n",
       "   1996,\n",
       "   2034,\n",
       "   4823,\n",
       "   3428,\n",
       "   8120,\n",
       "   2046,\n",
       "   1996,\n",
       "   2600,\n",
       "   1005,\n",
       "   1050,\n",
       "   1005,\n",
       "   4897,\n",
       "   2534,\n",
       "   1997,\n",
       "   4476,\n",
       "   102,\n",
       "   2297,\n",
       "   6902,\n",
       "   21543,\n",
       "   1010,\n",
       "   8507,\n",
       "   8507,\n",
       "   6902,\n",
       "   21543,\n",
       "   2297,\n",
       "   8799,\n",
       "   1010,\n",
       "   4937,\n",
       "   4937,\n",
       "   8799,\n",
       "   2325,\n",
       "   12136,\n",
       "   3790,\n",
       "   5132,\n",
       "   2316,\n",
       "   1010,\n",
       "   1996,\n",
       "   2703,\n",
       "   1996,\n",
       "   2703,\n",
       "   12136,\n",
       "   3790,\n",
       "   5132,\n",
       "   2316,\n",
       "   2703,\n",
       "   12136,\n",
       "   3790,\n",
       "   1010,\n",
       "   3505,\n",
       "   25584,\n",
       "   1010,\n",
       "   3449,\n",
       "   6371,\n",
       "   3387,\n",
       "   1010,\n",
       "   2928,\n",
       "   6583,\n",
       "   6199,\n",
       "   11475,\n",
       "   2078,\n",
       "   1010,\n",
       "   11120,\n",
       "   7779,\n",
       "   1010,\n",
       "   5006,\n",
       "   16273,\n",
       "   1010,\n",
       "   1998,\n",
       "   3520,\n",
       "   3913,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2073,\n",
       "   2106,\n",
       "   1996,\n",
       "   2801,\n",
       "   1997,\n",
       "   25742,\n",
       "   5092,\n",
       "   2497,\n",
       "   2272,\n",
       "   2013,\n",
       "   102,\n",
       "   2021,\n",
       "   2119,\n",
       "   1997,\n",
       "   2122,\n",
       "   2020,\n",
       "   2904,\n",
       "   1010,\n",
       "   2004,\n",
       "   1996,\n",
       "   2171,\n",
       "   2001,\n",
       "   2525,\n",
       "   11749,\n",
       "   2098,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2106,\n",
       "   4586,\n",
       "   2317,\n",
       "   2991,\n",
       "   1999,\n",
       "   2293,\n",
       "   2007,\n",
       "   102,\n",
       "   1996,\n",
       "   28984,\n",
       "   2709,\n",
       "   2000,\n",
       "   2037,\n",
       "   9151,\n",
       "   1998,\n",
       "   2424,\n",
       "   4586,\n",
       "   2317,\n",
       "   9428,\n",
       "   2757,\n",
       "   1010,\n",
       "   2108,\n",
       "   2921,\n",
       "   1999,\n",
       "   1037,\n",
       "   2331,\n",
       "   10359,\n",
       "   22889,\n",
       "   29440,\n",
       "   2011,\n",
       "   1996,\n",
       "   9947,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2043,\n",
       "   2106,\n",
       "   1996,\n",
       "   2034,\n",
       "   29583,\n",
       "   11238,\n",
       "   3042,\n",
       "   2272,\n",
       "   2041,\n",
       "   102,\n",
       "   1996,\n",
       "   29583,\n",
       "   2707,\n",
       "   6305,\n",
       "   2003,\n",
       "   1037,\n",
       "   18856,\n",
       "   13596,\n",
       "   18223,\n",
       "   4684,\n",
       "   3042,\n",
       "   7609,\n",
       "   2011,\n",
       "   29583,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2043,\n",
       "   2515,\n",
       "   4869,\n",
       "   1996,\n",
       "   6261,\n",
       "   2161,\n",
       "   1018,\n",
       "   2792,\n",
       "   2410,\n",
       "   2272,\n",
       "   2041,\n",
       "   102,\n",
       "   10999,\n",
       "   5829,\n",
       "   2046,\n",
       "   1996,\n",
       "   6992,\n",
       "   11231,\n",
       "   13331,\n",
       "   2160,\n",
       "   1010,\n",
       "   2021,\n",
       "   2044,\n",
       "   18255,\n",
       "   4978,\n",
       "   19327,\n",
       "   1010,\n",
       "   1996,\n",
       "   2048,\n",
       "   2131,\n",
       "   2046,\n",
       "   1037,\n",
       "   9685,\n",
       "   6685,\n",
       "   1998,\n",
       "   10999,\n",
       "   5829,\n",
       "   2067,\n",
       "   2041,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2043,\n",
       "   2001,\n",
       "   2886,\n",
       "   2006,\n",
       "   16537,\n",
       "   2161,\n",
       "   1015,\n",
       "   2207,\n",
       "   102,\n",
       "   2044,\n",
       "   2849,\n",
       "   2378,\n",
       "   1005,\n",
       "   1055,\n",
       "   4686,\n",
       "   9020,\n",
       "   2000,\n",
       "   3102,\n",
       "   2019,\n",
       "   19470,\n",
       "   16537,\n",
       "   1010,\n",
       "   1037,\n",
       "   16537,\n",
       "   2007,\n",
       "   1037,\n",
       "   12320,\n",
       "   2303,\n",
       "   3402,\n",
       "   3544,\n",
       "   2013,\n",
       "   1996,\n",
       "   2157,\n",
       "   12205,\n",
       "   2012,\n",
       "   2152,\n",
       "   3177,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2626,\n",
       "   2196,\n",
       "   2022,\n",
       "   2894,\n",
       "   2011,\n",
       "   13218,\n",
       "   27916,\n",
       "   102,\n",
       "   1036,\n",
       "   1036,\n",
       "   4845,\n",
       "   1999,\n",
       "   2293,\n",
       "   1005,\n",
       "   1005,\n",
       "   27916,\n",
       "   1062,\n",
       "   15630,\n",
       "   7317,\n",
       "   19092,\n",
       "   5671,\n",
       "   1062,\n",
       "   15630,\n",
       "   7317,\n",
       "   19092,\n",
       "   28774,\n",
       "   27235,\n",
       "   1017,\n",
       "   1024,\n",
       "   4805,\n",
       "   1019,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2040,\n",
       "   2038,\n",
       "   1996,\n",
       "   2087,\n",
       "   3275,\n",
       "   10080,\n",
       "   4386,\n",
       "   6665,\n",
       "   102,\n",
       "   9171,\n",
       "   3275,\n",
       "   24789,\n",
       "   2031,\n",
       "   2180,\n",
       "   2093,\n",
       "   6665,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2862,\n",
       "   1997,\n",
       "   9159,\n",
       "   1999,\n",
       "   1037,\n",
       "   2338,\n",
       "   2003,\n",
       "   2170,\n",
       "   102,\n",
       "   1999,\n",
       "   2715,\n",
       "   2808,\n",
       "   2788,\n",
       "   2006,\n",
       "   1996,\n",
       "   2310,\n",
       "   25301,\n",
       "   1997,\n",
       "   1996,\n",
       "   2516,\n",
       "   3931,\n",
       "   1010,\n",
       "   2021,\n",
       "   1999,\n",
       "   2070,\n",
       "   2808,\n",
       "   2872,\n",
       "   2012,\n",
       "   1996,\n",
       "   2203,\n",
       "   1006,\n",
       "   2156,\n",
       "   2067,\n",
       "   3043,\n",
       "   1007,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2043,\n",
       "   2515,\n",
       "   5180,\n",
       "   2265,\n",
       "   2039,\n",
       "   1999,\n",
       "   8410,\n",
       "   8124,\n",
       "   102,\n",
       "   1996,\n",
       "   3035,\n",
       "   2003,\n",
       "   2397,\n",
       "   2005,\n",
       "   8410,\n",
       "   1005,\n",
       "   1055,\n",
       "   2836,\n",
       "   1010,\n",
       "   2029,\n",
       "   19753,\n",
       "   1996,\n",
       "   2977,\n",
       "   1012,\n",
       "   102]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a81c236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='48303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/48303 00:00 < 4:05:04, 3.28 it/s, Epoch 0.00/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1196, in forward\n    outputs = self.roberta(\n              ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 844, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 529, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n    self_attention_outputs = self.attention(\n                             ^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n    self_outputs = self.self(\n                   ^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 276, in forward\n    context_layer = torch.matmul(attention_probs, value_layer)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.92 GiB total capacity; 10.13 GiB already allocated; 19.44 MiB free; 10.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 26\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     evaluation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmAP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model,\n\u001b[1;32m     19\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1196, in forward\n    outputs = self.roberta(\n              ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 844, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 529, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 413, in forward\n    self_attention_outputs = self.attention(\n                             ^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 340, in forward\n    self_outputs = self.self(\n                   ^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home2/kapilrk04/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py\", line 276, in forward\n    context_layer = torch.matmul(attention_probs, value_layer)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.92 GiB total capacity; 10.13 GiB already allocated; 19.44 MiB free; 10.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=9,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b3726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb947d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ab2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
