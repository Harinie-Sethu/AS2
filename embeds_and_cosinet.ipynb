{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kapil\\anaconda3\\envs\\anlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING TOKENIZED DATASETS AND EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_train = pickle.load(open('wiki_qa_train.pkl', 'rb'))\n",
    "wiki_qa_test = pickle.load(open('wiki_qa_test.pkl', 'rb'))\n",
    "wiki_qa_validation = pickle.load(open('wiki_qa_validation.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train = pickle.load(open('squad_train.pkl', 'rb'))\n",
    "squad_valid = pickle.load(open('squad_valid.pkl', 'rb'))\n",
    "\n",
    "numberbatch_embeddings = pickle.load(open('numberbatch_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wiki_Qa\n",
    "CODE USED TO GENERATE EMBEDDINGS IS PROVIDED BELOW:\n",
    "\n",
    "embeddings generated: https://drive.google.com/file/d/1fNQH8n4OfHGFBc6fyCI4UHuNYW7umqYX/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_vocab = [\"<PAD>\", \"<UNK>\", \"<S>\", \"</S>\"]\n",
    "\n",
    "def add_to_vocab(data, vocab):\n",
    "    for datum in data:\n",
    "        for words in datum[\"question\"]:\n",
    "            if words not in vocab:\n",
    "                vocab.append(words)\n",
    "        \n",
    "        for answer in datum[\"answers\"]:\n",
    "            for words in answer:\n",
    "                if words not in vocab:\n",
    "                    vocab.append(words)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_vocab(wiki_qa_train, wiki_qa_vocab)\n",
    "add_to_vocab(wiki_qa_test, wiki_qa_vocab)\n",
    "add_to_vocab(wiki_qa_validation, wiki_qa_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_vocab2id = {word: i for i, word in enumerate(wiki_qa_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling embeddings for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_embedding(word, word_embeddings):\n",
    "    possible_matches = []\n",
    "\n",
    "    index = len(word) - 1\n",
    "\n",
    "    while(index > 0):\n",
    "        for known_word in word_embeddings.keys():\n",
    "            if known_word.startswith(word[:index]):\n",
    "                possible_matches.append(known_word)\n",
    "        \n",
    "        if possible_matches:\n",
    "            avg_embedding = np.mean([word_embeddings[word] for word in possible_matches], axis=0)\n",
    "            return avg_embedding\n",
    "\n",
    "        index -= 1\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting embeddings for wiki_qa vocabulary, and converting each wiki_qa dataset word to its corresponding vocabulary ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22993/22993 [15:29<00:00, 24.74it/s] \n"
     ]
    }
   ],
   "source": [
    "wiki_qa_embeddings = []\n",
    "index = 0\n",
    "for word in tqdm(wiki_qa_vocab):\n",
    "   if(word==\"<PAD>\"):\n",
    "      wiki_qa_embeddings.append(torch.Tensor([0.0] * 300))\n",
    "   elif(word==\"<S>\"):\n",
    "      wiki_qa_embeddings.append(torch.Tensor([1.0] * 300))\n",
    "   elif(word==\"</S>\"):\n",
    "      wiki_qa_embeddings.append(torch.Tensor([-1.0] * 300))\n",
    "   elif(word==\"<UNK>\"):\n",
    "      wiki_qa_embeddings.append(torch.Tensor([0.0] * 300))\n",
    "   else:\n",
    "      if word in numberbatch_embeddings:\n",
    "         wiki_qa_embeddings.append(torch.Tensor(numberbatch_embeddings[word]))\n",
    "      else:\n",
    "         if(get_missing_embedding(word, numberbatch_embeddings) is not None):\n",
    "            wiki_qa_embeddings.append(torch.Tensor(get_missing_embedding(word, numberbatch_embeddings)))\n",
    "         else:\n",
    "            wiki_qa_embeddings.append(torch.Tensor([0.0] * 300))\n",
    "   index += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wiki_qa_embeddings, \"wiki_qa_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING SAVED EMBEDDINGS (IF ALREADY GENERATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qa_embeddings = torch.load(\"wiki_qa_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_qa_embeddings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    for i, datum in enumerate(data):\n",
    "        data[i][\"question\"] = [wiki_qa_vocab2id[word] for word in datum[\"question\"]]\n",
    "        data[i][\"answers\"] = [[wiki_qa_vocab2id[word] for word in answer] for answer in datum[\"answers\"]]\n",
    "\n",
    "    return data\n",
    "\n",
    "wiki_qa_train = tokenize_data(wiki_qa_train)\n",
    "wiki_qa_test = tokenize_data(wiki_qa_test)\n",
    "wiki_qa_validation = tokenize_data(wiki_qa_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD\n",
    "\n",
    "Similar code can be used to generate vocabulary, indices and embeddings for SQuAD. Will we implemented when we start training the model for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_vocab = [\"<PAD>\", \"<UNK>\", \"<S>\", \"</S>\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
